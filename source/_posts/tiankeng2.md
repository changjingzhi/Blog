---
title: 填坑——经典网络结构——AlexNet
date: 2024-04-23 10:48:00
tags: 填坑
---

## 问题1，卷积是什么？作用什么？
卷积（Convolution）是一种数学运算，常用于信号处理和图像处理领域。在信号处理中，卷积用于将输入信号与卷积核（也称为滤波器）进行运算，产生输出信号。
卷积的作用有以下几个方面：
1. 信号滤波：卷积可以用于信号滤波，通过将输入信号与合适的卷积核进行卷积运算，可以实现对信号的滤波操作。滤波可以用于去除信号中的噪声、平滑信号、强调信号中的某些频率成分等。
2. 特征提取：在图像处理中，卷积可以用于特征提取。通过将图像与不同的卷积核进行卷积运算，可以提取出图像中的不同特征，例如边缘、纹理、角点等。这些特征可以用于图像识别、目标检测和图像处理中的其他任务。
3. 信号压缩：卷积可以用于信号压缩。通过将输入信号与适当的卷积核进行卷积运算，可以将信号表示转换为另一种表示形式，通常具有更紧凑的表示。这种表示形式可以用于信号压缩和数据压缩。
4. 卷积神经网络：卷积神经网络（Convolutional Neural Network，CNN）是一种基于卷积运算的深度学习模型，广泛应用于图像识别、计算机视觉和自然语言处理等领域。卷积在 CNN 中用于提取图像或文本的特征，并通过多层卷积和池化操作来实现对输入数据的高级表示和分类。如果输入数据为图片，那么卷积层的作用就是提取图片中的信息，这些信息被称为图像特征，这些特征是由图像中的每个像素通过组合或者独立的方式所体现，比如图片的纹理特征、颜色特征、空间特征。

关于卷积其实还有很多问题，比如说输入一张（3x255x255）的图片，输入后经过卷积后输出的特征图大小为什么shape。1x1卷积为什么可以实现升维和降维。）

## 问题2，池化是什么？作用是什么？
池化（Pooling）是一种常用的操作，通常与卷积神经网络（CNN）结合使用。池化操作通过对输入数据的局部区域进行聚合或采样来减小数据的空间尺寸，从而减少参数数量、降低计算量，并提取出输入数据的重要特征。

池化的作用有以下几个方面
1. 降采样：池化操作可以减小输入数据的空间尺寸，从而降低后续层的计算复杂度。通过降低数据的维度，池化可以在保留重要特征的同时减少冗余信息，提高计算效率。
2. 平移不变性：池化操作具有一定的平移不变性。在图像处理中，通过对局部区域进行池化操作，可以使得输入图像在平移、旋转和缩放等变换下具有一定的不变性。这对于图像识别和目标检测等任务是有益的。
3. 特征提取：池化操作可以提取输入数据的重要特征。通过对局部区域进行池化，池化操作会选择区域中的最大值（最大池化）或平均值（平均池化）作为输出值，从而提取出输入数据的显著特征。这有助于减少数据的维度，并保留重要的特征信息。
4. 减少过拟合：池化操作可以在一定程度上减少过拟合。通过减小数据的空间尺寸，池化操作可以降低模型的参数数量，从而减少过拟合的风险。此外，池化操作还可以通过丢弃一些冗余信息来提高模型的泛化能力。

池化的种类
1. 最大池化（Max Pooling）：最大池化是一种常见的池化操作。在最大池化中，输入数据的局部区域被分割成不重叠的块，然后在每个块中选择最大值作为输出。最大池化可以提取出输入数据的显著特征，同时减小数据的空间尺寸。
2. 平均池化（Average Pooling）：平均池化是另一种常见的池化操作。在平均池化中，输入数据的局部区域被分割成不重叠的块，然后计算每个块中元素的平均值作为输出。平均池化可以平滑输入数据并减小数据的空间尺寸。
3. 自适应池化（Adaptive Pooling）：自适应池化是一种具有灵活性的池化操作。与最大池化和平均池化不同，自适应池化不需要指定池化窗口的大小，而是根据输入数据的尺寸自动调整池化窗口的大小。这使得自适应池化可以适应不同尺寸的输入数据。
4. 全局池化（Global Pooling）：全局池化是一种特殊的池化操作，它将整个输入数据的空间尺寸缩减为一个单一的值或向量。全局池化可以通过对输入数据的所有位置进行池化操作，从而提取出输入数据的全局特征。常见的全局池化有全局平均池化（Global Average Pooling）和全局最大池化（Global Max Pooling）。
## 问题3，全连接是什么？作用是什么？
## 问题4，AlexNet论文使用的loss函数是什么？
## 问题5，AlexNet论文中使用的梯度优化方式是什么，梯度怎么实现下降？
## 问题6，AlexNet论文中使用的评价指标是什么？
## 问题7，AlexNet中的创新点是什么？
1. ReLU激活函数的引入，采样非线性单元（ReLU）的深度卷积神经网络训练时间要比tanh单元要快几倍。而时间开销是进行模型训练过程中的很重要的因数。同时ReLU有效的防止了过拟合的现象。
2. 层叠池化操作，以往池化的大小PoolingSize与步长stride一般是相等的，例如：图像大小为256*256，PoolingSize=2×2，stride=2，这样可以使图像或是FeatureMap大小缩小一倍变为128，此时池化过程没有发生层叠。但是AlexNet采用了层叠池化操作，即PoolingSize > stride。这种操作非常像卷积操作，可以使相邻像素间产生信息交互和保留必要的联系。论文中也证明，此操作可以有效防止过拟合的发生。
3. Dropout操作， Dropout操作会将概率小于0.5的每个隐层神经元的输出设为0，即去掉了一些神经节点，达到防止过拟合。那些“失活的”神经元不再进行前向传播并且不参与反向传播。这个技术减少了复杂的神经元之间的相互影响。在论文中，也验证了此方法的有效性。
4. 网络层数更深，与原始的LeNet相比，AlexNet网络结构更深，LeNet为5层，AlexNet为8层。在随后的神经网络发展过程中，AlexNet逐渐让研究人员认识到网络深度对性能的巨大影响。当然，这种思考的重要节点出现在VGG网络（下一篇博文VGG论文中将会讲到）。
## 问题8，优化函数的具体实现是什么？
## 问题9，关于卷积后特征图应该怎么计算？
## 问题10，什么是过拟合合和欠拟合？
## 问题11，论文中怎么证明，层叠池化可以有效防止过拟合的发生？
## 问题12， 神经元数量和参数量的计算方法是什么？
## 问题13， softMax的机制是怎么样的？
## 问题14， AlexNet论文中使用的评估指标错误率的计算方法是什么？
## 问题15，Dropout的运行机制是什么，论文中怎么证明他们有效(理论证明和实验证明)？
## 问题16， 什么是超参数？
## 问题17， 什么是监督学习和无监督学习，半监督学习？


