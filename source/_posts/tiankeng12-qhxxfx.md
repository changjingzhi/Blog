---
title: 填坑——强化学习复习
date: 2024-06-09 17:50:57
tags: 填坑
---
为什么要系统的学习，因为网络上的博客大多是散装的，不系统，一个名词解释一大堆不同的解释。

人生中充满选择，每次选择就是一次决策，我们正是从一次次决策中，把自己带领到人生的下一段旅程中。在回忆往事时，我们会对生命中某些时刻的决策印象深刻。

## 什么是强化学习

![](pic/tk-qhxxfx2.png)  

1. 多臂老虎机
在多臂老虎机问题中，设计策略时就需要平衡探索和利用的次数，使得累积奖励最大化。一个比较常用的思路是在开始时做比较多的探索，在对每根拉杆都有比较准确的估计后，再进行利用。目前已有一些比较经典的算法来解决这个问题，例如E-贪婪算法、上置信界算法和汤普森采样算法等，我们接下来将分别介绍这几种算法。

2. 马尔可夫决策

随机过程： 随机现象在某时刻t的取值是一个向量随机变量，使用St表示，所有可能的状态集合S，St 通常取决于t时刻之前的状态。

马尔可夫性质： 某时刻的之态只取决于上一时刻的状态。

马尔可夫过程： （马尔可夫链） 指具有马尔可夫性质的随机过程，< S, P> S表示有限的状态集合，P表示转移矩阵

马尔可夫奖励过程（M）在马尔可夫过程的基础上加入奖励函数 和折扣因子，就可以得到马尔可夫奖励过程（Markov reward process）。一个马尔可夫奖励过程由构成（S，P，r，y），各个组成元素的含义如下所示。

S是有限状态的集合。
P是状态转移矩阵。
r是奖励函数，某个状态的奖励 指转移到该状态时可以获得奖励的期望。
y是折扣因子（discount factor），的取值范围为。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的更关注长期的累计奖励，接近 0 的更考虑短期奖励。

马尔可夫决策过程（Markov decision process，MDP）是强化学习的重要概念。
将这个来自外界的刺激称为智能体（agent）的动作，在马尔可夫奖励过程（MRP）的基础上加入动作，就得到了马尔可夫决策过程（MDP）。马尔可夫决策过程由元组构成，其中：

![](pic/tk-qhxxfx3.png) 


3. 动态规划算法和时序差分算法


动态规划算法，策略迭代和价值迭代。

| |
| :------ | 
|![策略迭代](pic/tk-qhxxfx-dg1.png)|
|![价值迭代](pic/tk-qhxxfx-dg2.png)|


在强化学习中，“模型”通常指与智能体交互的环境模型，即对环境的状态转移概率和奖励函数进行建模(有没有预先建立数学模型对真实环境进行抽象)。根据是否具有环境模型，强化学习算法分为两种：基于模型的强化学习（model-based reinforcement learning）和无模型的强化学习（model-free reinforcement learning）。无模型的强化学习根据智能体与环境交互采样到的数据直接进行策略提升或者价值估计，
 

时序差分是一种用来估计一个策略的价值函数的方法，它结合了蒙特卡洛和动态规划算法的思想。时序差分方法和蒙特卡洛的相似之处在于可以从样本数据中学习，不需要事先知道环境；和动态规划的相似之处在于根据贝尔曼方程的思想，利用后续状态的价值估计来更新当前状态的价值估计。
时序差分算法Sarsa和Q-learning算法。
| |
| :------ | 
|![Sarsa算法流程](pic/tk-qhxxfx-Sarsa.png)|
|![Q-learning算法流程](pic/tk-qhxxfx-qlearning.png)|

Sarsa 和 Q-learning 算法，便是两种无模型的强化学习方法.在基于模型的强化学习中，模型可以是事先知道的，也可以是根据智能体与环境交互采样到的数据学习得到的，然后用这个模型帮助策略提升或者价值估计。策略迭代和价值迭代，则是基于模型的强化学习方法，在这两种算法中环境模型是事先已知的。

Dyna-Q 算法是一个经典的基于模型的强化学习算法。如图 6-1 所示，Dyna-Q 使用一种叫做 Q-planning 的方法来基于模型生成一些模拟数据，然后用模拟数据和真实数据一起改进策略。Q-planning 每次选取一个曾经访问过的状态，采取一个曾经在该状态下执行过的动作，通过模型得到转移后的状态以及奖励，并根据这个模拟数据，用 Q-learning 的更新方式来更新动作价值函数。

| |
| :------ | 
|![](pic/tk-qhxxfx4.png)|
|![](pic/tk-qhxxfx5.png)|


4. DQN算法
环境的种类： 连续的环境（连续的离散）和离散的环境。

DQN：实验环境Cart，动作为离散。 DQN基于Q-learning，所以深度神经网络来近似Q价值函数，增加目标网络和经验回放。DQN网络是离线算法。
经验回放，在学习过过程中设置一个回放缓冲区来存储以往的数据经验，保存有每次从环境中采样中得到的（状态，动作，奖励，下一动作），在训练过程中会随机采样。也就是说，算法的更新不会直接和环境进行交互，
目标网络，是DQN中除Q网络外的另外一个深度神经网络，用于提供稳定的目标更新防止参数更新时产生的震荡。主网络（Q网络）参数定期或以较慢的学习率更新目标网络参数。



| |
| :------ | 
|![](pic/tk-qhxxfx6.png)|
|![](pic/tk-qhxxfx7.png)|

Double DQN 和 Dueling DQN。
Double DQN 行为网络：选择动作、 目标网络：评估行为。 降低值函数的过高偏差。

Dueling DQN： 

5. 策略梯度算法
 Q-learning、DQN 及 DQN 改进算法都是基于价值（value-based）的方法，其中 Q-learning 是处理有限状态的算法，而 DQN 可以用来解决连续状态的问题。在强化学习中，除了基于值函数的方法，还有一支非常经典的方法，那就是基于策略（policy-based）的方法。对比两者，基于值函数的方法主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略；而基于策略的方法则是直接显式地学习一个目标策略。基于策略的方法首先需要将策略参数化。假设目标策略是一个随机性策略，并且处处可微，其中是对应的参数。我们可以用一个线性模型或者神经网络模型来为这样一个策略函数建模，输入某个状态，然后输出一个动作的概率分布。我们的目标是要寻找一个最优策略并最大化这个策略在环境中的期望回报。
REINFORCE 算法是一个在线策略算法，之前收集到的轨迹数据不会被再次利用。
![REINFORCE算法具体流程](pic/tk-qhxxfx10.png)


![on-policy表示在线](pic/tk-qhxxfx8.png)
在线策略（on-policy）算法表示行为策略和目标策略是同一个策略；而离线策略（off-policy）算法表示行为策略和目标策略不是同一个策略。离线策略算法能够重复使用过往训练样本，往往具有更小的样本复杂度。


6.  Actor-Critic 算法
策略梯度算法变体——（融合学习价值函数和学习策略函数）基于值函数的方法（DQN）和基于策略的方法（REINFORCE），其中基于值函数的方法只学习一个价值函数，而基于策略的方法只学习一个策略函数。那么，一个很自然的问题是，有没有什么方法既学习价值函数，又学习策略函数呢？答案就是 Actor-Critic。Actor-Critic 是囊括一系列算法的整体架构，Actor-Critic 算法本质上是基于策略的算法，因为这一系列算法的目标都是优化一个带参数的策略，只是会额外学习价值函数，从而帮助策略函数更好地学习。
Actor-Critic 分为两个部分：Actor（策略网络）和 Critic（价值网络）。

Actor 要做的是与环境交互，并在 Critic 价值函数的指导下用策略梯度学习一个更好的策略。
Critic 要做的是通过 Actor 与环境交互收集的数据学习一个价值函数，这个价值函数会用于判断在当前状态什么动作是好的，什么动作不是好的，进而帮助 Actor 进行策略更新。



7. TRPO 算法
策略梯度算法变体—— (离散动作)——（在线策略算法）在策略迭代过程中加了一个约束，确保新旧策略的差异在一个受控范围内。
在更新时找到一块信任区域（trust region），在这个区域上更新策略时能够得到某种策略性能的安全性保证，这就是信任区域策略优化（trust region policy optimization，TRPO）算法的主要思想。TRPO 算法在 2015 年被提出，它在理论上能够保证策略学习的性能单调性，并在实际应用中取得了比策略梯度算法更好的效果。

TRPO 使用泰勒展开近似、共轭梯度、线性搜索等方法直接求解。



8. PPO算法-2017年
策略梯度算法变体——（离散动作）——（在线策略算法）—— 基于TRPO提出，目标函数简单又高效，实现了TRPO类似的性能
PPO 的优化目标与 TRPO 相同,PPO 用了一些相对简单的方法来求解。具体来说，PPO 有两种形式，一是 PPO-惩罚，二是 PPO-截断。

| |
| :------ | 
|![](pic/tk-qhxxf-ppo1.png)|
|![](pic/tk-qhxxfx-ppo2.png)|

9. 深度确定性策略梯度（deep deterministic policy gradient，DDPG）算法
策略梯度算法变体——（连续动作）——（离线策略算法）——结合DQN的离线经验池和AC的连续动作空间。特点：经验回放，目标网络软更新，添加连续噪声，在动作输入前批标准化Q网络。 训练不稳定，收敛性差，对超参数敏感，难适应不同复杂环境。


| |
| :------ | 
|![DDPG中的网络示意图](pic/tk-qhxxfx-DDPG.png)|
|![DDPG算法流程](pic/tk-qhxxfx-DDPG2.png)|

10. SAC算法
策略梯度算法变体—— （连续动作）——（离线策略算法）基于Soft Q-learning，都属于最大熵强化学习的范畴。
![SAC算法流程](pic/tk-qhxxfx-sac.png)
（注：由于SAC算法处理的是与连续动作交互的环境，策略网络输出一个高斯分布的均值和标准差来表示动作分布；而价值网络的输入是状态和动作的拼接向量，输出一个实数来表示动作价值。 ）

































































## 题目

1. 关于强化学习描述错误的是： C 

A. 强化学习的目标是最大化累计奖励。
B. 强化学习在某种程度上感知环境的状态。
C. 强化学习属于无监督学习。
D. 强化学习通过从交互中学习来实现目标。

强化学习的基本概念：agent：智能体。 enviroment： 环境. goal : 目标.  state： 状态. action： 行动. reward ： 奖励.   
第一层：  agent：执行动作的主体。 environment：强化学习所处的环境。 goal：强化学习的目标。 强化学习是agent在与环境的互动过程中为了达成一个目标而进行的学习过程。

第二层：  state：当前agent的状态。 action：执行的动作/行为。 reward：执行动作得到的实时的奖励。 state和action的循环往复过程构成了强化学习的主体部分。
需要注意的是：reward与 goal不是同一个概念，reward是执行某一个动作之后得到的实时的奖励，goal是强化学习最终的目标（一般来说使reward之和最大），但goal决定了reward。

第三层： 也是核心元素，包括两个函数，价值函数（Value function)和策略函数(Policy function）。

价值函数： Value function： 价值函数分为两种，一种是V状态价值函数（state value function),一种是Q状态行动函数（state action value function)。Q值评估的是动作的价值，代表agent做了这个动作之后一直到最终状态奖励总和的期望值；V值评估的是状态的价值，代表agent在这个状态下一直到最终状态的奖励总和的期望。价值越高，表示我从当前状态到最终状态能获得的平均奖励将会越高，因此我选择价值高的动作就可以了。   通常情况下来说，状态价值函数V是针对特定策略定义的，因为计算奖励的期望值取决于选取各个action的概率。状态行动函数V表面上与策略policy没什么关系，他取决于状态转移概率，但在强化学习中状态转移函数一般不变。要注意，Q值和V值之间可以互相转化。

策略函数：  Policy决定了某个state下应该选取哪一个action，也就是说，状态时Policy的输入，action是Policy的输出。策略Policy为每一个动作分配概率，例如：π(s1|a1) = 0.3，说明在状态s1下选择动作a1的概率是0.3,而该策略只依赖于当前的状态，不依赖于以前时间的状态，因此整个过程也是一个马尔可夫决策过程。强化学习的核心和训练目标就是要选择一个合适的Policy/,使得reward之和最大。

![](pic/tk-qhxxfx.png)    

2.  在强化学习中，智能体与环境交互产生的数据分布称为什么？ （C）

A. 策略函数
B. 状态转移
C. 占用度量
D. 奖励信号

占用度量是强化学习中智能体与环境交互产生的数据分布的术语。
强化学习中有一个关于数据分布的概念，叫作占用度量（occupancy measure），归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的状态动作对（state-action pair）的概率分布。

根据占用度量这一重要的性质，我们可以领悟到强化学习本质的思维方式。

强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。

由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。

占用度量有一个很重要的性质：给定两个策略及其与一个动态环境交互得到的两个占用度量，那么当且仅当这两个占用度量相同时，这两个策略相同。也就是说，如果一个智能体的策略有所改变，那么它和环境交互得到的占用度量也会相应改变。
[详细解释](https://zhuanlan.zhihu.com/p/660569825)


3. 强化学习的目标是什么？ (B)

A. 最小化累积奖励
B. 最大化累积奖励
C. 保持奖励不变
D. 随机选择奖励

强化学习的目标是最大化累积奖励，即智能体在多轮交互过程中获得的总奖励的期望值，将goal获取最高的值，而对于单轮的reward可以不一定是最优良的。

4. 下列哪项不是强化学习中的智能体关键要素 （A）

A. 惩罚
B. 奖励
C. 感知
D. 决策

强化学习中的智能体关键要素包括感知、决策和奖励，不包括惩罚。

感知。智能体在某种程度上感知环境的状态，从而知道自己所处的现状。例如，下围棋的智能体感知当前的棋盘情况；无人车感知周围道路的车辆、行人和红绿灯等情况；机器狗通过摄像头感知面前的图像，通过脚底的力学传感器来感知地面的摩擦功率和倾斜度等情况。

智能体根据当前的状态计算出达到目标需要采取的动作的过程叫作决策。例如，针对当前的棋盘决定下一颗落子的位置；针对当前的路况，无人车计算出方向盘的角度和刹车、油门的力度；针对当前收集到的视觉和力觉信号，机器狗给出4条腿的齿轮的角速度。策略是智能体最终体现出的智能形式，是不同智能体之间的核心区别。

奖励。环境根据状态和智能体采取的动作，产生一个标量信号作为奖励反馈。这个标量信号衡量智能体这一轮动作的好坏。例如，围棋博弈是否胜利；无人车是否安全、平稳且快速地行驶；机器狗是否在前进而没有摔倒。最大化累积奖励期望是智能体提升策略的目标，也是衡量智能体策略好坏的关键指标。


5. 以下关于策略的说法哪个是正确的？ （A）

A. 以上都是

B. 策略分为确定策略和随机性策略

C. 策略是状态到行为的映射

D. 随机性策略是状态s下产生的行为的概率分布


6. 在强化学习中，智能体的策略更新对以下哪些方面有直接影响？ （A，C,E）

A. 累积奖励
B. 状态转移概率
C. 与环境交互产生的数据分布
D. 智能体的感知能力
E. 占用度量

智能体的策略更新会影响其与环境交互产生的数据分布，占用度量，以及累积奖励。状态转移概率通常由环境本身决定，而智能体的感知能力是由其设计决定的。

为什么不会影响状态转移概率？
主要是因为状态转移概率是由环境的动态决定的，而不是由智能体的策略直接决定的。

在马尔可夫决策过程（MDP）中，状态转移概率 \( P(s'|s, a) \) 表示在状态 \( s \) 下采取行动 \( a \) 后转移到状态 \( s' \) 的概率。这一概率描述了环境在特定状态和行动下的动态行为。


7. 下列哪些描述正确地解释了强化学习中的奖励信号？（B、D、E）
A. 奖励信号是智能体做出决策时的输入
B. 奖励信号是环境根据状态和动作产生的标量信号
C. 奖励信号与智能体的累积奖励无关
D. 奖励信号是智能体策略优化的目标
E. 奖励信号衡量智能体动作的好坏

A、奖励reward是一个单一数值，即标量； B、奖励reward是衡量当前动作的好坏的一个标准   C、错。因为智能体做决策的输入是状态state，奖励是环境的输出   D、智能体的策略是获得更大的奖励，所以是它优化的目标   E、。因为累积奖励来自于单个奖励，二者是有强相关的。


8. 强化学习中，智能体与环境交互涉及哪些关键要素？（A、C、E）
A. 奖励
B. 惩罚
C. 决策
D. 动作
E. 感知


9. 对于ϵ-greedy策略探索方式，更高的ϵ 优于更低ϵ，更能让算法获得的最终奖励值。 B错
A. 对
B. 错

ϵ的值只会决定策略的收敛快慢，对于最终的收敛结果并没有一定的强相关联系。

10.  在强化学习中，智能体的策略改变会导致其与环境交互产生的数据分布发生改变。A对
A. 对
B. 错

智能体的策略改变会影响其与环境交互的方式，因此产生的数据分布也会发生相应的改变。

11.  强化学习中的智能体只需要考虑当前的奖励，而不需要考虑未来的状态变化。 B错
A. 对
B. 错

强化学习中的智能体不仅要考虑当前的奖励，还需要考虑未来的状态变化和累积奖励。、


12.  强化学习的数据分布是固定不变的，与智能体的策略无关。 B错
A. 对
B. 错

强化学习中的数据分布是随着智能体策略的不同而变化的，因为智能体的决策会影响其与环境交互产生的数据。

13. 机器学习可以分为预测型和决策性，有监督学习和无监督学习属于预测型，强化学习属于决策型。 A对
A. 对
B. 错


14. 基于模型的强化学习和模型无关的强化学习的根本区别在于学习过程中有没有__（环境）__模型。

基于模型的强化学习 (Model-based Reinforcement Learning)

**定义：**  
基于模型的强化学习方法利用一个环境的模型来进行学习和决策。这个模型描述了环境的动态，包括状态转移概率和奖励函数。

**主要特点：**
1. **环境模型：** 需要构建或学习一个模型来估计状态转移概率 \(P(s'|s, a)\) 和奖励函数 \(R(s, a)\)。
2. **规划：** 使用环境模型来模拟和规划未来的决策，通过预测不同动作的结果来选择最优动作。这通常包括算法如动态规划（Dynamic Programming）和蒙特卡罗树搜索（Monte Carlo Tree Search）。
3. **效率：** 可以通过利用模型进行规划和模拟来提高数据效率，减少对实际环境交互的需求。

**优点：**
- 能够通过模拟来减少实际环境中的试验和错误，尤其在实际交互代价高昂或危险的情况下。
- 更高的数据效率，因为模型能够提供额外的训练数据。

**缺点：**
- 构建准确的环境模型可能很困难，尤其在复杂或不确定的环境中。
- 如果模型不准确，可能会导致次优甚至错误的决策。

模型无关的强化学习 (Model-free Reinforcement Learning)

**定义：**  
模型无关的强化学习方法不使用环境的模型，而是直接从与环境的交互中学习最佳策略。

**主要特点：**
1. **直接学习：** 通过与环境的直接交互，利用奖励信号来学习最优策略或最优值函数。这包括方法如值函数逼近（如Q学习和SARSA）和策略优化（如策略梯度和近端策略优化PPO）。
2. **无模型：** 不需要了解或学习环境的状态转移概率和奖励函数，直接从经验中学习。

**优点：**
- 更加简单，不需要额外的建模步骤。
- 在高度复杂或不确定的环境中更为实用，因为无需准确建模环境。

**缺点：**
- 数据效率较低，需要大量的环境交互来学习有效的策略。
- 训练过程可能较长，尤其在大型或连续状态空间中。


15. 在多臂老虎机问题中，ε-贪心算法在每一步选择动作时，以概率__ε___进行探索，以概率___1-ε___进行利用。 

ε-贪心算法在每一时刻采取动作时，以概率ε随机选择一根拉杆进行探索，以概率1-ε选择以往经验中期望奖励估值最大的那根拉杆进行利用。这种策略平衡了探索新选项与利用已知最佳选项之间的关系。

16. 强化学习中的价值（value）是指智能体在与环境交互过程中获得的________的期望。 （整体回报;总体回报;折扣回报）

在强化学习中，价值是指智能体在与环境交互过程中每一轮获得的奖励信号累加形成的总回报的期望，有时会对未来回报进行打折，此时整体回报变为折扣回报。

17.   ε-贪心算法中，ε值随时间________，以平衡探索与利用。 （衰减;减小;变小）

ε-贪心算法中，ε值（探索概率）随时间衰减，以逐渐从探索转向利用。

18. 价值函数是用于评估给定策略下_______的好坏。 （状态;state）

般不特别说明时，价值函数指V(s)，所以它是评估状态s的好坏的尺度。


状态价值函数是指什么？ （C）

A.一个随机过程
B.一个策略
C.从某个状态出发的未来累积奖励的期望
D. 一个状态转移矩阵

状态价值函数是马尔可夫奖励过程中一个状态的期望回报。


在MDP中，策略（Policy）表示什么？ （C）

A. 动作的集合
B. 状态的集合
C. 在输入状态情况下采取动作的概率
D. 状态转移函数

策略是一个函数，表示在输入状态情况下采取动作的概率。


在马尔可夫奖励过程中，折扣因子的取值范围是多少？
A. (-∞, 0]
B. [0, +∞)
C. (0, 1)
D. [-1, 1]

折扣因子的取值范围为(0, 1)

下列哪项不是蒙特卡洛方法的特点？ （C）
A. 基于概率统计的数值计算方法
B. 使用重复随机抽样
C. 需要知道状态转移函数和奖励函数
D. 通过抽样结果归纳目标的数值估计

蒙特卡洛方法不需要知道MDP的状态转移函数和奖励函数，它可以得到一个近似值。

蒙特卡洛方法通过随机抽样来估计状态价值函数，采样次数越多，估计的结果越准确。 （A、对）
A. 对
B. 错

动态规划算法可以用于求解马尔可夫奖励过程中的价值函数，而蒙特卡洛方法则不适用于此。（B、错）
A. 对
B. 错

蒙特卡洛方法也可以用于估计马尔可夫奖励过程中的价值函数，且不需要知道状态转移函数和奖励函数。

马尔可夫性质指的是当前状态只取决于上一时刻的状态，与历史状态无关。 （A对）
A. 对
B. 错

虽然当前状态只与上一时刻的状态有关，但上一时刻的状态包含了之前所有状态的信息。


最优策略是相对于MDP而言的，不同的MDP可能有不同的最优策略。（A、对）
A. 对
B. 错

最优策略是针对特定的MDP而言的，不同的MDP环境下最优策略可能会有所不同。

1. DQN的输出层用什么激活函数? (A)
A.不需要激活函数，因为Q值可正可负，没有限值范围
B.用sigmoid激活函数，因为Q值介于0和1之间
C.用ReLU激活函数，用为Q值非负
D.用softmax激活函数，因为DQN的输出是—个概率分布

答案解析:DQN的输出表示回报的期望，它根据环境不同，值也不同，并没有限值范围，所以无需激活函数限制范围。


2. DQN的高估问题是由____引起的。(B)
A.DQN网络结构的缺陷。
B.Q学习算法的缺陷
C.更新频率
D.深度

答案解析:Q学习算法会被当前学习样本所影响而导致高估

3. 动作价值函数是_____的期望 (B)
A.奖励
B.回报
C.状态
D.动作

答案解析:动作价值函数，一般称作Q函数，它是回报的期望，它的值的大小用来衡量未来回报的高与低。


1. 在策略学习过程中，往往需要进行新策略探索与旧策略的利用，其目的分别是什么? (A)
A.尝试不同策略，以进行策略提升/提升对旧策略的评估能力
B.提高策略多样性/让旧策略朝着最优策略的方向进行优化



1. (判断题)蒙特卡洛方法的好处是"无偏性”，所以在训练价值网络时使用它可以加速训练过程。 (A 对)
A.对
B.错






## 问题

1. 基于模型和和无模型（Model Free）的算法分布有哪些？

基于模型（值迭代和策略迭代）的动态规划，Dyna-Q算法。

无模型（Model-Free), 回合更新——蒙特卡洛方法。 时间差分，基于价值的在线策略Sarsa，离线策略Q-learning，由Q-learning改进的DQN。 基于策略，策略梯度的Actor-Critic，DDPG


2. 贝尔曼方程的推导及应用，价值迭代算法和策略迭代算法的具体步骤和收敛性分析


| |
| :------ | 
|![](pic/tk-qhxxfx-bemfc1.png)|
|![](pic/tk-qhxxfx-bemfc2.png)|
|![](pic/tk-qhxxfx-bemfc3.png)|


3. Q-;earning和Sarsa之间的区别

| |
| :------ | 
|![](pic/tk-qhxxfx-tiandati.png)|
|![](pic/tk-qhxxfx-Actor-Crit.png)|